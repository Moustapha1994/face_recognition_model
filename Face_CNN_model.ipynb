{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données contiennent des images de visage recadrées de 16 personnes réparties en formation et en test. Nous allons entraîner le modèle CNN en utilisant les images du dossier Training, puis tester le modèle en utilisant les images invisibles du dossier de test, pour vérifier si le modèle est capable de reconnaître le numéro de visage des images invisibles ou non.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 244 images belonging to 16 classes.\n",
      "Found 244 images belonging to 16 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'face1': 0,\n",
       " 'face10': 1,\n",
       " 'face11': 2,\n",
       " 'face12': 3,\n",
       " 'face13': 4,\n",
       " 'face14': 5,\n",
       " 'face15': 6,\n",
       " 'face16': 7,\n",
       " 'face2': 8,\n",
       " 'face3': 9,\n",
       " 'face4': 10,\n",
       " 'face5': 11,\n",
       " 'face6': 12,\n",
       " 'face7': 13,\n",
       " 'face8': 14,\n",
       " 'face9': 15}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modèle CNN d'apprentissage profond pour reconnaître le visage\n",
    "'''Ce script utilise une base de données d'images et crée un modèle CNN dessus pour tester\n",
    "   si l'image donnée est reconnue correctement ou non'''\n",
    " \n",
    "'''########PRÉTRAITEMENT D'IMAGES pour les données de FORMATION et de TEST#######'''\n",
    " \n",
    "# Spécifier le dossier où les images sont présentes\n",
    "TrainingImagePath='./Face_Images/Face Images/Final Training Images'\n",
    "TestImagePath='Face_Images/Face Images/Final Testing Images/face13/2face13.jpg'\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# En savoir plus sur ImageDataGenerator au lien ci-dessous\n",
    "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    " \n",
    "# Définition des transformations de prétraitement sur les images raw des données d'entraînement\n",
    "# Ces hyperparamètres permettent de générer des versions légèrement tordues\n",
    "# de l'image originale, ce qui conduit à un meilleur modèle, car il apprend\n",
    "# sur le bon et le mauvais mélange d'images\n",
    "train_datagen = ImageDataGenerator(\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True)\n",
    " \n",
    "# Définition des transformations de prétraitement sur les images raw des données de test\n",
    "# Aucune transformation n'est effectuée sur les images de test\n",
    "test_datagen = ImageDataGenerator()\n",
    " \n",
    "# Générer les données d'entraînement\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    " \n",
    " \n",
    "# Générer les données de test\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    " \n",
    "# Affichage d'étiquettes de classe pour chaque face\n",
    "test_set.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création d'un mappage pour les noms d'index et de visage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dictionnaire class_index ci-dessus a des noms de visage comme clés et le mappage numérique comme valeurs. Nous devons l'échanger, car le modèle de classificateur renverra la réponse sous forme de mappage numérique et nous devons en extraire le nom de visage.\n",
    "\n",
    "De plus, puisqu'il s'agit d'un problème de classification multi-classes, nous comptons le nombre de faces uniques, car cela sera utilisé comme le nombre de neurones de sortie dans la couche de sortie du classificateur ANN entièrement connecté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of Face and its ID {0: 'face1', 1: 'face10', 2: 'face11', 3: 'face12', 4: 'face13', 5: 'face14', 6: 'face15', 7: 'face16', 8: 'face2', 9: 'face3', 10: 'face4', 11: 'face5', 12: 'face6', 13: 'face7', 14: 'face8', 15: 'face9'}\n",
      "\n",
      " The Number of output neurons:  16\n"
     ]
    }
   ],
   "source": [
    "'''############ Creating lookup table for all faces ############'''\n",
    "# class_indices have the numeric tag for each face\n",
    "TrainClasses=training_set.class_indices\n",
    " \n",
    "# Storing the face and the numeric tag for future reference\n",
    "ResultMap={}\n",
    "for faceValue,faceName in zip(TrainClasses.values(),TrainClasses.keys()):\n",
    "    ResultMap[faceValue]=faceName\n",
    " \n",
    "# Saving the face map for future reference\n",
    "import pickle\n",
    "with open(\"ResultsMap.pkl\", 'wb') as fileWriteStream:\n",
    "    pickle.dump(ResultMap, fileWriteStream)\n",
    " \n",
    "# The model will give answer as a numeric tag\n",
    "# This mapping will help to get the corresponding face name for it\n",
    "print(\"Mapping of Face and its ID\",ResultMap)\n",
    " \n",
    "# The number of neurons for the output layer is equal to the number of faces\n",
    "OutputNeurons=len(ResultMap)\n",
    "print('\\n The Number of output neurons: ', OutputNeurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du modèle de reconnaissance faciale CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'extrait de code ci-dessous, j'ai créé un modèle CNN avec\n",
    "\n",
    "2 couches cachées de convolution\n",
    "\n",
    "2 couches cachées de pooling maximum\n",
    "\n",
    "1 couche d'aplatissement\n",
    "\n",
    "1 couche ANN cachée\n",
    "\n",
    "1 couche de sortie avec 16 neurones (un pour chaque face)\n",
    "\n",
    "Vous pouvez augmenter ou diminuer la convolution, la mise en commun maximale, les couches ANN cachées et le nombre de neurones qu'elles contiennent.\n",
    "\n",
    "\n",
    "Gardez simplement à l'esprit que plus vous ajoutez de couches / neurones, plus le modèle est lent.\n",
    "\n",
    "\n",
    "De plus, lorsque vous avez une grande quantité d'images, de l'ordre de 50K et plus, le processeur de votre ordinateur portable peut ne pas être efficace pour apprendre ces nombreuses images. Vous devrez vous procurer un ordinateur portable compatible GPU ou utiliser des services cloud comme AWS ou Google Cloud.\n",
    "\n",
    "\n",
    "Étant donné que les données que nous avons utilisées pour la démonstration sont petites et ne contiennent que 244 images pour la formation, vous pouvez l'exécuter facilement sur votre ordinateur portable \n",
    "\n",
    "\n",
    "Outre la sélection du meilleur nombre de couches et du nombre de neurones qu'il contient, pour chaque couche, certains hyper paramètres doivent également être réglés.\n",
    "\n",
    "\n",
    "Jetez un coup d'œil à certains des hyperparamètres importants\n",
    "\n",
    "\n",
    "Filtres = 32: ce nombre indique le nombre de filtres que nous utilisons pour regarder les pixels de l'image pendant l'étape de convolution. Certains filtres peuvent capturer des bords nets, certains filtres peuvent capturer des variations de couleur, certains filtres peuvent capturer des contours, etc. En fin de compte, nous obtenons des informations importantes à partir des images. Dans la première couche, le nombre de filtres = 32 est couramment utilisé, puis augmentant la puissance de 2. Comme dans la couche suivante, il est de 64, dans la couche suivante, il est de 128 et ainsi de suite.\n",
    "\n",
    "kernel_size = (5,5) : Ceci indique la taille de la fenêtre glissante pendant la convolution, dans cette étude de cas, nous utilisons une fenêtre glissante de 5X5 pixels.\n",
    "\n",
    "strides = (1, 1): À quelle vitesse ou lentement la fenêtre coulissante doit-elle se déplacer pendant la convolution. Nous utilisons le réglage le plus bas de 1X1 pixels. Cela signifie faire glisser la fenêtre de convolution de 5X5 (kernal_size) de 1 pixel sur l'axe x et de 1 pixel sur l'axe y jusqu'à ce que l'image entière soit balayée.\n",
    "\n",
    "input_shape = (64,64,3): Les images ne sont rien d'autre qu'une matrice de codes de couleur RVB. lors de notre prétraitement des données, nous avons compressé les images en 64X64, donc la forme attendue est 64X64X3. Signifie 3 tableaux de 64X64, un pour les couleurs RVB chacun.\n",
    "\n",
    "kernel_initializer = 'uniform' : Lorsque les neurones commencent leur calcul, un algorithme doit décider de la valeur de chaque poids. Ce paramètre spécifie cela. Vous pouvez choisir différentes valeurs comme «normal» ou «glorot_uniform».\n",
    "\n",
    "activation = 'relu' : Ceci spécifie la fonction d'activation pour les calculs à l'intérieur de chaque neurone. Vous pouvez choisir des valeurs telles que «relu», «tanh», «sigmoid», etc.\n",
    "\n",
    "optimizer = 'adam':  Ce paramètre permet de trouver les valeurs optimales de chaque poids dans le réseau neuronal. 'adam' est l'un des optimiseurs les plus utiles, un autre est 'rmsprop'\n",
    "\n",
    "batch_size = 10 : Ceci spécifie combien de lignes seront transmises au réseau en une fois, après quoi le calcul SSE commencera et le réseau neuronal commencera à ajuster ses poids en fonction des erreurs.\n",
    "\n",
    "Lorsque toutes les lignes sont passées dans les lots de 10 lignes chacun comme spécifié dans ce paramètre, nous appelons cette 1-epoch. Ou un cycle de données complet. Ceci est également connu sous le nom de descente de gradient mini-batch. Une petite valeur de batch_size obligera le LSTM à regarder les données lentement, comme 2 lignes à la fois ou 4 lignes à la fois, ce qui pourrait entraîner un surajustement, par rapport à une valeur élevée comme 20 ou 50 lignes à la fois, ce qui faire en sorte que le LSTM examine rapidement les données, ce qui pourrait entraîner un sous-ajustement. Par conséquent, une valeur appropriée doit être choisie en utilisant le réglage des hyperparamètres.\n",
    "Epoques = 10 : La même activité d'ajustement des poids se poursuit 10 fois, comme spécifié par ce paramètre. En termes simples, le LSTM examine les données d'entraînement complètes 10 fois et ajuste ses poids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - ETA: 0s - loss: 150.1828 - accuracy: 0.0724WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n",
      "8/8 [==============================] - 15s 2s/step - loss: 145.5473 - accuracy: 0.0730 - val_loss: 3.2061 - val_accuracy: 0.0820\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.9366 - accuracy: 0.0692\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.5147 - accuracy: 0.2301\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 10s 1s/step - loss: 2.0280 - accuracy: 0.3836\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.2899 - accuracy: 0.5926\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.9493 - accuracy: 0.6811\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.5733 - accuracy: 0.8551\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.3959 - accuracy: 0.8920\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.3214 - accuracy: 0.9140\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 0.1700 - accuracy: 0.9480\n",
      "Total Time Taken:  2 Minutes\n"
     ]
    }
   ],
   "source": [
    "'''######################## Creation du model CNN deep learning  ########################'''\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    " \n",
    "'''Initialisation du reseau de neurone convolutif'''\n",
    "classifier= Sequential()\n",
    " \n",
    "''' Etape--1 Convolution\n",
    "# Ajout de la première couche de CNN\n",
    "# nous utilisons le format (64,64,3) car nous utilisons le backend TensorFlow\n",
    "# Cela signifie 3 matrice de taille (64X64) pixels représentant les composants Rouges, Verts et Bleus des pixels\n",
    "'''\n",
    "classifier.add(Convolution2D(32, kernel_size=(5, 5), strides=(1, 1), input_shape=(64,64,3), activation='relu'))\n",
    " \n",
    "'''# SEtape--2 MAX Pooling'''\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    " \n",
    "'''# COUCHE SUPPLÉMENTAIRE de CONVOLUTION pour une meilleure précision '''\n",
    "classifier.add(Convolution2D(64, kernel_size=(5, 5), strides=(1, 1), activation='relu'))\n",
    " \n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "'''# Etape--3 FLattening'''\n",
    "classifier.add(Flatten())\n",
    " \n",
    "'''# Etape--4 Fully Connected Neural Network'''\n",
    "classifier.add(Dense(64, activation='relu'))\n",
    " \n",
    "classifier.add(Dense(OutputNeurons, activation='softmax'))\n",
    " \n",
    "'''# Compiling the CNN'''\n",
    "#classificateur.compiler(perte='binary_crossentropy', optimiseur='adam', métriques=['précision'])\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])\n",
    " \n",
    "###########################################################\n",
    "import time\n",
    "# Measuring the time taken by the model to train\n",
    "StartTime=time.time()\n",
    " \n",
    "# Starting the model training\n",
    "history = classifier.fit_generator(\n",
    "                    training_set,                    \n",
    "                    epochs=10,\n",
    "                    validation_data=test_set,\n",
    "                    validation_steps=10)\n",
    "\n",
    "# classifier.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history = classifier.fit(TrainingImagePath, training_set, epochs=10, \n",
    "#                     validation_data=(test_image, test_set))\n",
    "\n",
    "EndTime=time.time()\n",
    "print(\"Total Time Taken: \", round((EndTime-StartTime)/60), 'Minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test du classificateur CNN sur des images invisibles\n",
    "En utilisant l'une des images du dossier de données de test, nous pouvons vérifier si le modèle est capable de reconnaître le visage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Prediction is:  face2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "''' Making single predictions '''\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    " \n",
    "TestImagePath='Face_Images/Face Images/Final Testing Images/face2/1face2.jpg'\n",
    "test_image=image.load_img(TestImagePath,target_size=(64, 64))\n",
    "test_image=image.img_to_array(test_image)\n",
    " \n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    " \n",
    "result=classifier.predict(test_image,verbose=0)\n",
    "#print(training_set.class_indices)\n",
    " \n",
    "print('####'*10)\n",
    "print('Prediction is: ',ResultMap[np.argmax(result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: 'Face_Images/Face Images/Final Testing Images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-a27e15ced9b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lower right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Face_Images/Face Images/Final Testing Images\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msupported\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m   \"\"\"\n\u001b[0;32m--> 299\u001b[0;31m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0m\u001b[1;32m    300\u001b[0m                         target_size=target_size, interpolation=interpolation)\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    111\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'Face_Images/Face Images/Final Testing Images'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "test=image.load_img(\"Face_Images/Face Images/Final Testing Images\",target_size=(64, 64))\n",
    "test_loss, test_acc = classifier.evaluate(test, test_set, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Vous pouvez modifier ce modèle pour créer un modèle de classification pour n'importe quel groupe d'images. Mettez simplement les images de chaque catégorie dans son dossier respectif et entraînez le modèle.\n",
    "\n",
    "L'algorithme CNN nous a aidés à créer de nombreuses applications géniales autour de nous! Facebook est l'exemple parfait! Il a formé son modèle DeepFace CNN sur des millions d'images et a une précision de 97% pour reconnaître n'importe qui sur Facebook. Cela peut même surpasser les humains! comme vous vous en souvenez seulement quelques visages \n",
    "\n",
    "CNN est également utilisé dans l'industrie médicale pour aider les médecins à obtenir une prédiction précoce du cancer bénin ou malin en utilisant les images de la tumeur. De même, faites-vous une idée de la typhoïde en regardant les images radiographiques, etc.\n",
    "\n",
    "Les usages de CNN sont nombreux et se développent rapidement autour de nous!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
